{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe9f50a1",
      "metadata": {
        "id": "fe9f50a1"
      },
      "source": [
        "# Bag of Words and Vector Representations - Assignment 3:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sham-nlp/2026nlp-3-bow-and-vector-representations/blob/main/03_bag_of_words_and_vector_representations_assignment_student.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "**Name:** `Your Name Here`\n",
        "\n",
        "**Date:** `Insert Date`\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This notebook is designed as hands-on practice for:\n",
        "- Bag of Words (BoW)\n",
        "- Embedding layers & sparsity in PyTorch\n",
        "- Semantic similarity (cosine) + simple visualization (PCA)\n",
        "- Challenge [Optional]: Training a small **Word2Vec CBOW** model on a toy dataset\n",
        "\n",
        "\n",
        "## Instructions\n",
        "1. Run the setup cell as it is. You do not need to change anything there.\n",
        "2. Complete the code sections marked with `# YOUR CODE HERE` and `# YOUR CODE IN _`.\n",
        "3. Run cells top-to-bottom in order.\n",
        "4. **Submission**: Submit this notebook with all cells executed and outputs visible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7446391b",
      "metadata": {
        "id": "7446391b"
      },
      "source": [
        "## Part 1: Setup (Toy Dataset + Helpers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21962d91",
      "metadata": {
        "id": "21962d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ebb823-3585-4ec5-9a3b-f0f8fdba5e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example sentence: Ramadan Kareem to everyone\n",
            "Tokens: ['ramadan', 'kareem', 'to', 'everyone']\n",
            "Number of sentences: 12\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run this cell first.\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For plots (Ex03)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42 # This value ensures that if you run the notebook again, you will get the same results :)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "toy_sentences = [\n",
        "    \"Ramadan Kareem to everyone\",\n",
        "    \"The month of Ramadan is here\",\n",
        "    \"Ramadan is a blessed month\",\n",
        "    \"I love cats\",\n",
        "    \"I hate cats\",\n",
        "    \"Cats are cute\",\n",
        "    \"I love coffee\",\n",
        "    \"Coffee is amazing\",\n",
        "    \"I enjoy machine learning\",\n",
        "    \"Deep learning is part of machine learning\",\n",
        "    \"I enjoy natural language processing\",\n",
        "    \"Natural language processing uses vectors\",\n",
        "]\n",
        "\n",
        "def simple_tokenize(text: str):\n",
        "    \"\"\"Lowercase + keep words only (simple tokenizer for teaching).\"\"\"\n",
        "    # We will study Tokenization in depth in a future session. For now, we get the tokens by splitting the words inside the sentence.\n",
        "    text = text.lower()\n",
        "    # keep alphabetic tokens\n",
        "    return re.findall(r\"[a-z]+\", text)\n",
        "\n",
        "tokenized = [simple_tokenize(s) for s in toy_sentences]\n",
        "\n",
        "print(\"Example sentence:\", toy_sentences[0])\n",
        "print(\"Tokens:\", tokenized[0])\n",
        "print(\"Number of sentences:\", len(toy_sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part 2: Bag of Words\n",
        "\n",
        "Goal: transform a sentence into a Bag-of-Words (count vector).\n",
        "\n",
        "You'll:\n",
        "1. Build a vocabulary from the toy dataset.\n",
        "2. Convert one sentence into a BoW vector.\n",
        "3. (Optional) Create BoW for *all* sentences.\n",
        "\n",
        "**Reminder:** BoW ignores word order."
      ],
      "metadata": {
        "id": "5l0HNuYe02yz"
      },
      "id": "5l0HNuYe02yz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1!</summary>\n",
        "\n",
        "Flatten `tokenized` into a single list of all tokens, then use `set()` for unique words.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2!</summary>\n",
        "\n",
        "```python\n",
        "all_tokens = [t for sent in tokenized for t in sent]\n",
        "vocab = list(set(all_tokens))\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "l40AFZ-iyIfN"
      },
      "id": "l40AFZ-iyIfN"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1: Build a vocabulary from the dataset.\n",
        "# For this exercise, you should include all unique tokens.\n",
        "#\n",
        "# Output:\n",
        "# - vocab: list of tokens\n",
        "# - word2idx: dict mapping token -> index. The idea is to assign an id (number) to each word. This index will be used as the order in the vocab list\n",
        "\n",
        "\n",
        "all_tokens = # YOUR CODE HERE # You can use the tokens from the tokenized list\n",
        "vocab = # YOUR CODE HERE\n",
        "word2idx = # YOUR CODE HERE\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"First 15 vocab items:\", vocab[:15])"
      ],
      "metadata": {
        "id": "pCTWWtMT02XP"
      },
      "id": "pCTWWtMT02XP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1!</summary>\n",
        "\n",
        "Create `torch.zeros(len(word2idx))`, then loop over tokens and increment at `word2idx[token]`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2!</summary>\n",
        "\n",
        "```python\n",
        "bow = torch.zeros(len(word2idx))\n",
        "for t in tokens:\n",
        "    bow[word2idx[t]] += 1\n",
        "return bow\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "-H8heD_4yO_w"
      },
      "id": "-H8heD_4yO_w"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2: Write a function that converts a tokenized sentence into a BoW vector (counts).\n",
        "#\n",
        "# Output:\n",
        "# - bow: torch tensor of shape [|V|] with counts\n",
        "\n",
        "def sentence_to_bow(tokens, word2idx):\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: create a zero vector, then increment indices for each token\n",
        "    return bow\n",
        "\n",
        "# Test on a sentence:\n",
        "test_sentence = \"Ramadan is a blessed month\"\n",
        "test_tokens = simple_tokenize(test_sentence)\n",
        "print(\"Test tokens:\", test_tokens)\n",
        "\n",
        "bow = sentence_to_bow(test_tokens, word2idx)\n",
        "print(bow)\n",
        "print(\"BoW shape:\", bow.shape)\n",
        "print(\"Non-zero entries:\", int((bow > 0).sum()))"
      ],
      "metadata": {
        "id": "O7FAsMjr2gsk"
      },
      "id": "O7FAsMjr2gsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1!</summary>\n",
        "Use sentence_to_bow to iterate over all tokens in `tokenized`\n",
        "See https://docs.pytorch.org/docs/stable/generated/torch.stack.html\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2!</summary>\n",
        "\n",
        "`torch.stack([sentence_to_bow(t, word2idx) for t in tokenized])`\n",
        "</details>"
      ],
      "metadata": {
        "id": "qICeYW67ym1j"
      },
      "id": "qICeYW67ym1j"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3 (Optional): Build a BoW matrix for all sentences.\n",
        "# Output:\n",
        "# - bow_matrix: tensor of shape [num_sentences, |V|]\n",
        "\n",
        "bow_matrix = # YOUR CODE HERE\n",
        "\n",
        "print(\"BoW matrix shape:\", bow_matrix.shape)"
      ],
      "metadata": {
        "id": "p79W7Ym023QG"
      },
      "id": "p79W7Ym023QG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part 3:  Embedding Layer, Vocab, and Sparse Gradients\n",
        "\n",
        "Goal: Understand how to:\n",
        "- define a vocabulary and token IDs\n",
        "- use `nn.Embedding`\n",
        "- compare gradients **with and without** `sparse=True`\n",
        "- observe how many embedding rows receive non-zero gradients\n",
        "\n",
        "**Key idea:** Only tokens that appear in the batch get gradients.  \n",
        "With `sparse=True`, gradients are stored sparsely (only updated rows), which can save memory for huge vocabularies.\n",
        "\n",
        "> Note: Not all optimizers support sparse gradients (e.g., use `SparseAdam` or `Adagrad`)."
      ],
      "metadata": {
        "id": "OQeM6gZb29xX"
      },
      "id": "OQeM6gZb29xX"
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll reuse vocab/word2idx from Part 01\n",
        "\n",
        "vocab = ['a', 'amazing', 'are', 'blessed', 'cats', 'coffee', 'cute', 'deep', 'enjoy', 'everyone', 'hate', 'here', 'i', 'is', 'kareem',\n",
        " 'language', 'learning', 'love', 'machine', 'month', 'natural', 'of', 'part', 'processing', 'ramadan', 'the', 'to', 'uses', 'vectors']\n",
        "\n",
        "word2idx= {'a': 0,\n",
        " 'amazing': 1,\n",
        " 'are': 2,\n",
        " 'blessed': 3,\n",
        " 'cats': 4,\n",
        " 'coffee': 5,\n",
        " 'cute': 6,\n",
        " 'deep': 7,\n",
        " 'enjoy': 8,\n",
        " 'everyone': 9,\n",
        " 'hate': 10,\n",
        " 'here': 11,\n",
        " 'i': 12,\n",
        " 'is': 13,\n",
        " 'kareem': 14,\n",
        " 'language': 15,\n",
        " 'learning': 16,\n",
        " 'love': 17,\n",
        " 'machine': 18,\n",
        " 'month': 19,\n",
        " 'natural': 20,\n",
        " 'of': 21,\n",
        " 'part': 22,\n",
        " 'processing': 23,\n",
        " 'ramadan': 24,\n",
        " 'the': 25,\n",
        " 'to': 26,\n",
        " 'uses': 27,\n",
        " 'vectors': 28}\n"
      ],
      "metadata": {
        "id": "Bl6kRtYxZ4ms"
      },
      "id": "Bl6kRtYxZ4ms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1!</summary>\n",
        "\n",
        "For each sentence, convert each token to its ID using `word2idx`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2!</summary>\n",
        "\n",
        "`[[word2idx[t] for t in sent] for sent in batch_tokens]`\n",
        "</details>"
      ],
      "metadata": {
        "id": "zsMpB_bczGMW"
      },
      "id": "zsMpB_bczGMW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pick a mini-batch of sentences and turn them into token IDs.\n",
        "batch_texts = [\n",
        "    \"ramadan is a blessed month\",\n",
        "    \"the month of ramadan is here\",\n",
        "    \"natural language processing uses vectors\",\n",
        "]\n",
        "\n",
        "batch_tokens = [simple_tokenize(s) for s in batch_texts]\n",
        "print(\"Batch tokens:\", batch_tokens)\n",
        "\n",
        "# TODO 2: Convert tokens -> IDs.\n",
        "\n",
        "batch_ids = # YOUR CODE HERE # Hint: You should use the idx2word you created earlier!\n",
        "\n",
        "print(\"Batch ids:\", batch_ids)"
      ],
      "metadata": {
        "id": "ZJQ2qC3R3Hr7"
      },
      "id": "ZJQ2qC3R3Hr7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "`num_embeddings=len(vocab)`, `embedding_dim=EMB_DIM`\n",
        "\n",
        "Docs: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "</details>"
      ],
      "metadata": {
        "id": "qj6OqeDqzeFn"
      },
      "id": "qj6OqeDqzeFn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: mean pooling to get a sentence vector from word vectors.\n",
        "def mean_pool(embeddings: torch.Tensor):\n",
        "    \"\"\"embeddings: [seq_len, dim] -> [dim]\"\"\"\n",
        "    return embeddings.mean(dim=0)\n",
        "\n",
        "# We'll compare embedding gradients with sparse=False vs sparse=True\n",
        "EMB_DIM = 16\n",
        "\n",
        "def run_embedding_grad_demo(sparse: bool):\n",
        "    emb = nn.Embedding(num_embeddings=# YOUR CODE HERE,\n",
        "                       embedding_dim=# YOUR CODE HERE,\n",
        "                       sparse=sparse)\n",
        "\n",
        "    # Create a single training step: predict a fake scalar target from sentence embeddings.\n",
        "    # We'll create sentence vectors by mean pooling and then score them with a linear layer.\n",
        "    scorer = nn.Linear(EMB_DIM, 1)\n",
        "\n",
        "    # Build a batch tensor: pad to same length for convenience\n",
        "    max_len = max(len(x) for x in batch_ids)\n",
        "    pad_id = word2idx[vocab[0]]  # any id (we'll mask it out); for real work use a PAD token.\n",
        "\n",
        "    batch = []\n",
        "    mask = []\n",
        "    for seq in batch_ids:\n",
        "        padded = seq + [pad_id] * (max_len - len(seq))\n",
        "        m = [1]*len(seq) + [0]*(max_len - len(seq))\n",
        "        batch.append(padded)\n",
        "        mask.append(m)\n",
        "\n",
        "    batch = torch.tensor(batch)         # [B, L]\n",
        "    mask = torch.tensor(mask).float()   # [B, L]\n",
        "\n",
        "    # Forward\n",
        "    word_vecs = emb(batch)              # [B, L, D]\n",
        "    # mask out padded positions:\n",
        "    word_vecs = word_vecs * mask.unsqueeze(-1)\n",
        "    sent_vecs = word_vecs.sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-9)  # [B, D]\n",
        "    scores = scorer(sent_vecs).squeeze(-1)  # [B]\n",
        "\n",
        "    # Fake regression target\n",
        "    target = torch.tensor([1.0, 0.0, 1.0])\n",
        "\n",
        "    loss = F.mse_loss(scores, target)\n",
        "    loss.backward()\n",
        "\n",
        "    grad = emb.weight.grad\n",
        "    return loss.item(), grad\n",
        "\n",
        "# TODO 3: Run both variants and inspect the gradient structure.\n",
        "\n",
        "loss_dense, grad_dense = run_embedding_grad_demo(sparse=False)\n",
        "loss_sparse, grad_sparse = run_embedding_grad_demo(sparse=True)\n",
        "\n",
        "print(\"Loss (dense): \", loss_dense)\n",
        "print(\"Loss (sparse):\", loss_sparse)\n",
        "\n",
        "print(\"\\nDense grad type:\", type(grad_dense), \"shape:\", grad_dense.shape)\n",
        "print(\"Sparse grad type:\", type(grad_sparse), \"is_sparse:\", getattr(grad_sparse, \"is_sparse\", False))\n"
      ],
      "metadata": {
        "id": "6lV-nYyt4V7k"
      },
      "id": "6lV-nYyt4V7k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1 (TODO 4)!</summary>\n",
        "\n",
        "`(row_sums > 0).sum().item()`\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2 (TODO 5)!</summary>\n",
        "\n",
        "`grad_sparse._indices()[0].tolist()`\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 3 (TODO 6)!</summary>\n",
        "\n",
        "`set(updated_rows_sparse) == set(torch.where(row_sums > 0)[0].tolist())`\n",
        "</details>"
      ],
      "metadata": {
        "id": "NGZj8T2JznAe"
      },
      "id": "NGZj8T2JznAe"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4: Count how many rows have non-zero gradients in the dense case.\n",
        "\n",
        "row_sums = grad_dense.abs().sum(dim=1)\n",
        "nonzero_rows_dense = # YOUR CODE HERE\n",
        "\n",
        "print(\"Non-zero gradient rows (dense):\", nonzero_rows_dense, \"out of\", len(vocab))\n",
        "\n",
        "# TODO 5: For sparse gradients, inspect which rows are updated.\n",
        "# HINT: grad_sparse._indices() gives the row indices\n",
        "\n",
        "updated_rows_sparse = # YOUR CODE HERE\n",
        "\n",
        "print(\"Updated rows (sparse):\", updated_rows_sparse)\n",
        "print(\"Number of updated rows (sparse):\", len(updated_rows_sparse))\n",
        "\n",
        "# TODO 6: Compare the sets of updated rows (dense vs sparse) - should match.\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5__AiBlGdYtW"
      },
      "id": "5__AiBlGdYtW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part 4: Semantic Similarity + Visualization (Cosine + PCA)\n",
        "\n",
        "Goal:\n",
        "1. Create **sentence vectors** (simple approach: mean pooling of word embeddings).\n",
        "2. Use **cosine similarity** to find the closest sentences to a query sentence (like a mini recommender).\n",
        "3. Visualize sentence vectors after **dimensionality reduction** (PCA to 2D).\n",
        "\n",
        "For this exercise, we will use a pretrained light glove model. Follow the instructions and run the cells to download the model\n",
        "\n",
        "**Important Note** If you face any problems during the model download (due to connection issue), you can use a raw Embedding layer from Pytorch. However, keep in mind that the results of the similarity can be very weak with this layer."
      ],
      "metadata": {
        "id": "-REztqhtd2Ex"
      },
      "id": "-REztqhtd2Ex"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Glove Model"
      ],
      "metadata": {
        "id": "lpO0MXIsegOe"
      },
      "id": "lpO0MXIsegOe"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "2O-HMfP4ekn-"
      },
      "id": "2O-HMfP4ekn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "metadata": {
        "id": "VgBWVkBYemnC"
      },
      "id": "VgBWVkBYemnC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "`model[word]` returns the embedding vector. Filter tokens that exist in the model:\n",
        "`[model[t] for t in tokens if t in model]`\n",
        "</details>"
      ],
      "metadata": {
        "id": "YXxDOCE50OIv"
      },
      "id": "YXxDOCE50OIv"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO 1: Create an embedding layer and compute a sentence embedding for each sentence in toy_sentences.\n",
        "\n",
        "def sentence_embedding(tokens):\n",
        "    vecs = # YOUR CODE HERE\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "sentence_vecs = np.stack([\n",
        "    sentence_embedding(simple_tokenize(s))\n",
        "    for s in toy_sentences\n",
        "])"
      ],
      "metadata": {
        "id": "yIC0Vlyjen_2"
      },
      "id": "yIC0Vlyjen_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint (TODO 2)!</summary>\n",
        "\n",
        "`cosine_similarity(sentence_vecs[query_idx:query_idx+1], sentence_vecs)[0]`\n",
        "\n",
        "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint (TODO 3)!</summary>\n",
        "\n",
        "`np.argsort(sims)[-k:][::-1]`\n",
        "</details>"
      ],
      "metadata": {
        "id": "OfsWpRCv0TOQ"
      },
      "id": "OfsWpRCv0TOQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2: Cosine similarity based retrieval.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_idx = 0\n",
        "sims = # YOUR CODE HERE\n",
        "\n",
        "print(\"Query:\", toy_sentences[query_idx])\n",
        "\n",
        "# TODO 3: get top-k indices\n",
        "\n",
        "top_idx = # YOUR CODE HERE\n",
        "for i in top_idx:\n",
        "    print(f\"{sims[i]:.3f} | {toy_sentences[i]}\")\n"
      ],
      "metadata": {
        "id": "Q_oFPfEeexEe"
      },
      "id": "Q_oFPfEeexEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "```python\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(sentence_vecs)\n",
        "```\n",
        "\n",
        "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "</details>"
      ],
      "metadata": {
        "id": "ckVOp0wH0fzd"
      },
      "id": "ckVOp0wH0fzd"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4: PCA visualization in 2D.\n",
        "# - Reduce sentence_vecs [N, D] to [N, 2]\n",
        "# - Plot points and label them with their sentence index\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pca = # YOUR CODE HERE\n",
        "X2 = # YOUR CODE HERE\n",
        "\n",
        "plt.scatter(X2[:,0], X2[:,1])\n",
        "for i, (x,y) in enumerate(X2):\n",
        "    plt.text(x+0.01, y+0.01, str(i))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for i, s in enumerate(toy_sentences):\n",
        "    print(i, \"->\", s)"
      ],
      "metadata": {
        "id": "kCM_cHLXgdCw"
      },
      "id": "kCM_cHLXgdCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Embedding Layer from Pytorch\n"
      ],
      "metadata": {
        "id": "gPFT0weufKeX"
      },
      "id": "gPFT0weufKeX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1 (vecs)!</summary>\n",
        "\n",
        "`emb(ids)` — pass the tensor through the embedding layer.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2 (sentence_vecs)!</summary>\n",
        "\n",
        "`torch.stack([sentence_embedding(t) for t in tokenized])`\n",
        "</details>"
      ],
      "metadata": {
        "id": "1pgrYdhU0qHo"
      },
      "id": "1pgrYdhU0qHo"
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll build sentence vectors using an embedding layer.\n",
        "# TODO 1: Create an embedding layer and compute a sentence embedding for each sentence in toy_sentences.\n",
        "\n",
        "EMB_DIM = 32\n",
        "emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=EMB_DIM)\n",
        "\n",
        "def sentence_to_ids(tokens):\n",
        "    return [word2idx[t] for t in tokens]\n",
        "\n",
        "def sentence_embedding(tokens):\n",
        "    ids = torch.tensor(sentence_to_ids(tokens))\n",
        "    vecs =  # YOUR CODE HERE      # [L, D]\n",
        "    sent_vec = vecs.mean(dim=0)  # [D]\n",
        "    return sent_vec\n",
        "\n",
        "# Build a matrix: [N, D]\n",
        "# TODO: compute sentence_vecs by stacking sentence_embedding(tokenized[i])\n",
        "sentence_vecs = # YOUR CODE HERE\n",
        "\n",
        "print(\"Sentence vectors shape:\", sentence_vecs.shape)"
      ],
      "metadata": {
        "id": "5OrvYlyWeBSP"
      },
      "id": "5OrvYlyWeBSP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1 (sims)!</summary>\n",
        "\n",
        "`F.cosine_similarity(query.unsqueeze(0), sentence_vecs, dim=1)`\n",
        "\n",
        "Docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2 (topk)!</summary>\n",
        "\n",
        "`torch.topk(sims, k=k)`\n",
        "</details>"
      ],
      "metadata": {
        "id": "l0sPwKEt0tAm"
      },
      "id": "l0sPwKEt0tAm"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2: Cosine similarity based retrieval.\n",
        "# Given a query sentence index, return the top-k most similar other sentences.\n",
        "\n",
        "def topk_similar(query_idx: int, k: int = 3):\n",
        "    query = sentence_vecs[query_idx]              # [D]\n",
        "    # TODO: compute cosine similarity between query and all sentence_vecs -> [N]\n",
        "    # HINT: F.cosine_similarity function can be used here\n",
        "\n",
        "    sims = # YOUR CODE HERE\n",
        "\n",
        "    # Exclude the query itself\n",
        "    sims = sims.clone()\n",
        "    sims[query_idx] = -1e9\n",
        "\n",
        "    # TODO: get top-k indices\n",
        "    vals, idxs = # YOUR CODE HERE\n",
        "\n",
        "    return idxs, vals\n",
        "\n",
        "query_idx = 0\n",
        "topk, scores = topk_similar(query_idx, k=4)\n",
        "print(\"Query:\", toy_sentences[query_idx])\n",
        "for i, s in zip(topk.tolist(), scores.tolist()):\n",
        "    print(f\"  sim={s:.3f} | {toy_sentences[i]}\")"
      ],
      "metadata": {
        "id": "LmXBQ7gteWu-"
      },
      "id": "LmXBQ7gteWu-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "```python\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(sentence_vecs.detach().numpy())\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "Dnli6BTO00Kr"
      },
      "id": "Dnli6BTO00Kr"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3: PCA visualization in 2D.\n",
        "# - Reduce sentence_vecs [N, D] to [N, 2]\n",
        "# - Plot points and label them with their sentence index\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pca = # YOUR CODE HERE\n",
        "X2 = # YOUR CODE HERE\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.scatter(X2[:, 0], X2[:, 1])\n",
        "\n",
        "for i, (x, y) in enumerate(X2):\n",
        "    plt.text(x + 0.01, y + 0.01, str(i), fontsize=9)\n",
        "\n",
        "plt.title(\"Sentence vectors (PCA to 2D) — indices correspond to toy_sentences\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Index -> sentence\")\n",
        "for i, s in enumerate(toy_sentences):\n",
        "    print(i, \"->\", s)"
      ],
      "metadata": {
        "id": "U7EO6OKren7l"
      },
      "id": "U7EO6OKren7l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part 5 Challenge! [Optional]: Train a Word2Vec Model (CBOW) on the Toy Dataset\n",
        "\n",
        "CBOW objective: **given context words, predict the center word**.\n",
        "\n",
        "We'll implement:\n",
        "- dataset creation: (context_ids, target_id)\n",
        "- a CBOW model with `nn.Embedding` + mean pooling + linear classifier\n",
        "- a training loop with cross entropy loss\n",
        "\n",
        "Because the dataset is tiny, don't expect perfect semantics, focus on the mechanics."
      ],
      "metadata": {
        "id": "Bf6e6wcSe869"
      },
      "id": "Bf6e6wcSe869"
    },
    {
      "cell_type": "code",
      "source": [
        "# You do not need to change this cell. And it is okay if you do not understand token-related codes for now. We'll cover them in later sessions inshaa Allah\n",
        "\n",
        "# Build CBOW training examples.\n",
        "WINDOW = 2 # We'll use a window size of 2: i.e. two words before and two words after the center word. See the following example:\n",
        "# For tokens: [w0, w1, w2, w3, w4]\n",
        "# Center= w2, context=[w0,w1,w3,w4]\n",
        "\n",
        "def build_cbow_examples(tokenized_sentences, window=2):\n",
        "    examples = []\n",
        "    for sent in tokenized_sentences:\n",
        "        ids = [word2idx[t] for t in sent]\n",
        "        for center in range(len(ids)):\n",
        "            left = max(0, center - window)\n",
        "            right = min(len(ids), center + window + 1)\n",
        "            context = [ids[i] for i in range(left, right) if i != center]\n",
        "            target = ids[center]\n",
        "            # Skip if context is empty (very short sentences)\n",
        "            if len(context) == 0:\n",
        "                continue\n",
        "            examples.append((context, target))\n",
        "    return examples\n",
        "\n",
        "examples = build_cbow_examples(tokenized, window=WINDOW)\n",
        "print(\"Number of CBOW examples:\", len(examples))\n",
        "print(\"First 5 examples (context_ids, target_id):\", examples[:5])\n",
        "\n",
        "# Make a simple collate function that pads contexts to same length in a batch.\n",
        "# Output:\n",
        "# - contexts: LongTensor [B, L]\n",
        "# - mask: FloatTensor [B, L] 1 for real tokens, 0 for padding\n",
        "# - targets: LongTensor [B]\n",
        "\n",
        "PAD_ID = word2idx[vocab[0]]  # (for teaching only; normally add an explicit <PAD> token)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    max_len = max(len(ctx) for ctx, _ in batch)\n",
        "    contexts, masks, targets = [], [], []\n",
        "    for ctx, tgt in batch:\n",
        "        pad_len = max_len - len(ctx)\n",
        "        contexts.append(ctx + [PAD_ID]*pad_len)\n",
        "        masks.append([1]*len(ctx) + [0]*pad_len)\n",
        "        targets.append(tgt)\n",
        "    return torch.tensor(contexts), torch.tensor(masks).float(), torch.tensor(targets)\n",
        "\n",
        "# Quick sanity check:\n",
        "contexts, masks, targets = collate_batch(examples[:4])\n",
        "print(\"contexts:\", contexts)\n",
        "print(\"masks:\", masks)\n",
        "print(\"targets:\", targets)"
      ],
      "metadata": {
        "id": "3D9NKED4fPYC"
      },
      "id": "3D9NKED4fPYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "`{idx: word for word, idx in word2idx.items()}`\n",
        "</details>"
      ],
      "metadata": {
        "id": "OzZ6SigQ06JT"
      },
      "id": "OzZ6SigQ06JT"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1: Create an idx2word mapping.\n",
        "# This should map each number to a word, based on word2idx\n",
        "idx2word = # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "SjWe9UDSa2Dp"
      },
      "id": "SjWe9UDSa2Dp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint 1 (forward)!</summary>\n",
        "\n",
        "```python\n",
        "embeddings = self.emb(context_ids)\n",
        "masked = embeddings * mask.unsqueeze(-1)\n",
        "pooled = masked.sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-9)\n",
        "return self.linear(pooled)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2 (optimizer/criterion)!</summary>\n",
        "\n",
        "`optimizer = torch.optim.Adam(model.parameters(), lr=0.01)`\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()`\n",
        "</details>"
      ],
      "metadata": {
        "id": "kHfgcVyb09Rv"
      },
      "id": "kHfgcVyb09Rv"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2: Implement the CBOW model.\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.linear = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_ids: torch.Tensor, mask: torch.Tensor):\n",
        "        # context_ids: [B, L]\n",
        "        # mask: [B, L] (1 for real, 0 for padding)\n",
        "\n",
        "        # TODO: look up embeddings -> [B, L, D]\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: mask out padding, then mean-pool over L -> [B, D]\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: logits = linear(pooled) -> [B, V]\n",
        "        # YOUR CODE HERE\n",
        "        return # YOUR CODE HERE\n",
        "\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMB_DIM = 32\n",
        "model = CBOW(VOCAB_SIZE, EMB_DIM)\n",
        "\n",
        "# TODO 3: Choose an optimizer and a loss.\n",
        "\n",
        "optimizer = # YOUR CODE HERE\n",
        "criterion = # YOUR CODE HERE\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "yWWYfslwfSFX"
      },
      "id": "yWWYfslwfSFX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "logits = model(contexts, masks)\n",
        "loss = criterion(logits, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "CGtV5k0N1Ak7"
      },
      "id": "CGtV5k0N1Ak7"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4: Training loop\n",
        "# Train for a few epochs and watch loss go down.\n",
        "\n",
        "def iterate_minibatches(data, batch_size=16, shuffle=True):\n",
        "    idxs = list(range(len(data)))\n",
        "    if shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for start in range(0, len(data), batch_size):\n",
        "        batch = [data[i] for i in idxs[start:start+batch_size]]\n",
        "        yield batch\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterate_minibatches(examples, batch_size=BATCH_SIZE, shuffle=True):\n",
        "        contexts, masks, targets = collate_batch(batch)\n",
        "\n",
        "        # TODO: reset the gradients of the optimizer\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: forward -> logits\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: compute loss\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: backward + step\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        total_loss += loss.item() * len(batch)\n",
        "\n",
        "    avg_loss = total_loss / len(examples)\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | avg_loss={avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "iqh9P62gfTwN"
      },
      "id": "iqh9P62gfTwN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Hint!</summary>\n",
        "\n",
        "`F.cosine_similarity(W, q.unsqueeze(0), dim=1)`\n",
        "</details>"
      ],
      "metadata": {
        "id": "agXt9GjG1D26"
      },
      "id": "agXt9GjG1D26"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 5: Inspect learned word neighbors (cosine similarity in embedding space).\n",
        "# We'll compute cosine similarity between a query word embedding and all other words.\n",
        "\n",
        "def most_similar_words(query_word: str, topk: int = 5):\n",
        "    query_id = word2idx[query_word]\n",
        "    W = model.emb.weight.detach()  # [V, D]\n",
        "    q = W[query_id]                # [D]\n",
        "\n",
        "    # TODO: cosine similarity between W and q -> [V]\n",
        "    sims = # YOUR CODE HERE\n",
        "\n",
        "    # Exclude itself\n",
        "    sims[query_id] = -1e9\n",
        "    vals, idxs = torch.topk(sims, k=topk)\n",
        "    return [(idx2word[i.item()], vals[j].item()) for j, i in enumerate(idxs)]\n",
        "\n",
        "# Try a few query words (if they exist in vocab)\n",
        "for w in [\"ramadan\", \"month\", \"cats\", \"coffee\", \"learning\"]:\n",
        "    if w in word2idx:\n",
        "        print(\"\\nQuery:\", w)\n",
        "        print(most_similar_words(w, topk=5))"
      ],
      "metadata": {
        "id": "hgFHFQMCfY36"
      },
      "id": "hgFHFQMCfY36",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lpO0MXIsegOe",
        "gPFT0weufKeX"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}